

##  Kettle整合大数据平台

###  Kettle整合Hadoop

#### Hadoop环境准备

1、查看hadoop的文件系统

- 通过浏览器访问

```html
http://node1:50070/
```

- 通过终端访问

```shell
hadoop fs -ls / # 查看文件
```

2、在hadoop文件系统中创建/hadoop/test目录

```shell
hadoop fs -mkdir -p /hadoop/test  
```

3、在本地创建1.txt

- vim 1.txt

```html
id,name
1,itheima
2,itcast
```

4、上传1.txt到hadoop文件系统的/hadoop/test目录

```shell
hadoop fs -put 1.txt /hadoop/test
```

#### kettle与hahoop环境整合

1、确保Hadoop的环境变量设置好HADOOP_USER_NAME为root

2、从hadoop下载核心配置文件

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/hdfs-site.xml
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/core-site.xml
```

文件会被下载到windows的下载目录

sz命令设置下载到windows的目录：

| linux下载文件到windows                                      |
| ----------------------------------------------------------- |
| ![image-20200205162500695](img/image-20200205162500695.png) |
| ![image-20200205162639526](img/image-20200205162639526.png) |



3、把hadoop核心配置文件(hdfs-site.xml和core-site.xml)放入kettle目录

```shell
data-integration\plugins\pentaho-big-data-plugin\hadoop-configurations\cdh514
```

4、修改 `data-integration\plugins\pentaho-big-data-plugin\plugin.properties`文件

- 修改plugin.properties

```shell
active.hadoop.configuration=cdh514
```

| plugin.propeties                                            |
| ----------------------------------------------------------- |
| ![image-20200205162801320](img/image-20200205162801320.png) |

5、 创建Hadoop clusters

具体步骤如下：

| 创建hadoop cluster                                          |
| ----------------------------------------------------------- |
| ![image-20200205162933991](img/image-20200205162933991.png) |
| ![image-20200205163018669](img/image-20200205163018669.png) |
| ![image-20200205163201486](img/image-20200205163201486.png) |



点击测试结果如图片右侧展示效果说明整合hadoop环境没有问题！！点击确定保存以上操作即可！！

查看链接是否保存：

| hadoop连接保存                                              |
| ----------------------------------------------------------- |
| ![image-20200205163327492](img/image-20200205163327492.png) |

如果与hadoop链接没有保存后续是无法操作hadoop集群！！

####  Hadoop file input组件

Kettle在Big data分类中提供了一个Hadoop file input 组件用来从hdfs文件系统中读取数据。

| hadoop file input                                           |
| ----------------------------------------------------------- |
| ![image-20200205163533264](img/image-20200205163533264.png) |





需求：

- 从Hadoop文件系统读取/hadoop/test/1.txt文件，把数据输入到Excel中。



实习步骤：

1、拖入以下组件

<img src="img/clip_image017-16468851811091.png" align="left" style="border:1px solid #999"/>

2、配置Hadoop File Input组件

指定hdfs的目标路径：

| hadoop file input                                           |
| ----------------------------------------------------------- |
| ![image-20200205164223439](img/image-20200205164223439.png) |



指定文件内容格式：

| 指定文件内容格式                                            |
| ----------------------------------------------------------- |
| ![image-20200109145528606](img/image-20200109145528606.png) |



点击字段查看获取字段是否正确：

| 获取字段                                                    |
| ----------------------------------------------------------- |
| ![image-20200205164439030](img/image-20200205164439030.png) |



配置excel输出组件：

| excel输出组件配置                                           |
| ----------------------------------------------------------- |
| ![image-20200205164529691](img/image-20200205164529691.png) |



点击excel输出组件获取字段查看字段是否正确：

| 获取字段                                                    |
| ----------------------------------------------------------- |
| ![image-20200205164624756](img/image-20200205164624756.png) |



启动转换任务：

| 执行任务                                                    |
| ----------------------------------------------------------- |
| ![image-20200205164718324](img/image-20200205164718324.png) |
| ![image-20200205164746715](img/image-20200205164746715.png) |
| 执行结果                                                    |
| ![image-20200205164818973](img/image-20200205164818973.png) |



#### Hadoop file output组件

Kettle在Big data分类中提供了一个Hadoop file output 组件用来向hdfs文件系统中保存数据

| hadoop file output组件                                       |
| ------------------------------------------------------------ |
| ![image-20200205163533264](img/image-20200205163533264-1580983912463.png) |



需求：

- 读取 user.json 把数据写入到hdfs文件系统的的/hadoop/test/2.txt中。

实现步骤：

1、拖入以下组件

| 组件配置                                                    |
| ----------------------------------------------------------- |
| ![image-20200205165008109](img/image-20200205165008109.png) |

2、配置 JSON 输入组件

| 指定json文件的路径                                          |
| ----------------------------------------------------------- |
| ![image-20200205165120462](img/image-20200205165120462.png) |
| 配置json input组件读取的字段                                |
| ![image-20200205165251370](img/image-20200205165251370.png) |







3、配置Hadoop file output组件

| 指定hdfs目标路径                                            |
| ----------------------------------------------------------- |
| ![image-20200205165351594](img/image-20200205165351594.png) |
| 指定源文件的属性信息：                                      |
| ![image-20200205165542753](img/image-20200205165542753.png) |
| ![image-20200205165629082](img/image-20200205165629082.png) |



- 问题

![image-20200205165908948](../../../../1上课共享资料/day01/1.讲义/assets/image-20200205165908948.png)

错误：用户没有权限

解决：

```shell
# 修改权限
  hadoop fs -chmod -R 777  /
```

### Kettle整合Hive

####  初始化数据

1、连接hive

| beeline连接hive               |
| ----------------------------- |
| ![img](img/clip_image039.jpg) |



2、创建并切换数据库

```sql
create database test;
use test;
```



3、创建表

```sql
create table a(
	a int,
    b int
)
row format delimited fields terminated by ',' stored as TEXTFILE;
show tables;
```



4、创建数据文件

```shell
vim a.txt
1,11
2,22
3,33
```



5、 从文件加载数据到表

```sql
load data local inpath '/root/a.txt' into table a;
```



6、查询表

```sql
select * from a;
```

#### kettle与Hive整合

1、从虚拟机下载Hadoop的jar包

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/share/hadoop/common/hadoop-common-2.6.0-cdh5.14.0.jar
```

2、把jar包放置在\data-integration\lib目录下

| 上传到lib目录下                                             |
| ----------------------------------------------------------- |
| ![image-20200205170115700](img/image-20200205170115700.png) |



3、重启kettle，重新加载生效

关掉之前打开的kettle重新启动！！

#### 从hive中读取数据

- hive数据库是通过jdbc来进行连接，可以通过表输入控件来获取数据。

需求：

- 从hive数据库的test库的a表中获取数据，并把数据保存到Excel中。

实现步骤：

1、设计一下kettle组件结构

选择输入文件夹内的表输入组件：

| 表输入组件                                                   |
| ------------------------------------------------------------ |
| ![image-20200205170433530](img/image-20200205170433530.png)  |
| <img src="img/clip_image047.png" align="left" style="border:1px solid #999"/> |



2、配置表输入组件

| 新建hivejdbc连接：                                          |
| ----------------------------------------------------------- |
| ![image-20200205170958301](img/image-20200205170958301.png) |
| ![image-20200205171055461](img/image-20200205171055461.png) |
| 配置excel输出组件                                           |
| ![image-20200205171150064](img/image-20200205171150064.png) |
| ![image-20200205171210756](img/image-20200205171210756.png) |



#### 把数据保存到hive数据库

hive数据库是通过jdbc来进行连接，可以通过表输出控件来保存数据。

需求：

- 从Excel资料\02.kettle测试数据\01.用户数据源\file_user.xls中读取数据，把数据保存在hive数据库的test数据库的t_user表。

实现步骤：

1、设计如下kettle组件结构

| 组件配置图                                                   |
| ------------------------------------------------------------ |
| <img src="img/clip_image054.png" align="left" style="border:1px solid #999"/> |



2、配置 Excel输入组件

| excele输入组件                                              |
| ----------------------------------------------------------- |
| ![image-20200205171342879](img/image-20200205171342879.png) |
| 查看excel解析字段是否正确                                   |
| ![image-20200205171512191](img/image-20200205171512191.png) |



2、配置表输出组件

| 表输出组件                                                   |
| ------------------------------------------------------------ |
| ![image-20200227152851902](img/image-20200227152851902.png)  |
| 获取流中的字段                                               |
| ![image-20200205173951319](img/image-20200205173951319.png)  |
| ![image-20200205174959098](img/image-20200205174959098-16468852344652.png) |
| 验证                                                         |
| ![image-20200205175138955](img/image-20200205175138955.png)  |



#### 执行Hive的HiveSQL语句

Kettle中可以执行Hive的HiveSQL语句，使用作业的SQL脚本。

需求：

- 聚合查询weblogs表（以IP和年月分组统计的PV数据），同时建立一个新表保存查询数据。

**准备hive表**

在hive的test数据库下创建weblogs表：

```sql
 CREATE TABLE `weblogs`(                           
  `client_ip` string,                              
  `full_request_date` string,                      
  `day` string,                                    
  `month` string,                                  
  `month_num` int,                                 
  `year` string,                                   
  `hour` string,                                   
  `minute` string,                                 
  `second` string,                                 
  `timezone` string,                               
  `http_verb` string,                              
  `uri` string,                                    
  `http_status_code` string,                       
  `bytes_returned` string,                         
  `referrer` string,                               
  `user_agent` string) 
  row format delimited fields terminated by '\t' stored as textfile;
```

导入资料\资料\02.kettle测试数据\hive-weblogs\下的数据

```shell
load data local inpath '/root/weblogs_parse.txt' into table weblogs;
```

验证数据

```sql
select * from test.weblogs limit 5;
```



实现步骤：

1、设计如下作业组件结构

| 设计如下作业组件结构                                        |
| ----------------------------------------------------------- |
| ![image-20200205175216309](img/image-20200205175216309.png) |
| ![image-20200205175313672](img/image-20200205175313672.png) |
| ![img](img/clip_image069.png)                               |





2、配置SQL组件

| 配置sql组件                                                  |
| ------------------------------------------------------------ |
| ![image-20200205175421659](img/image-20200205175421659.png)  |
| ![image-20200205175421659](assets/image-20200205175421659.png) |



3、测试数据是否生成

| 验证数据                                                    |
| ----------------------------------------------------------- |
| ![image-20200205175518614](img/image-20200205175518614.png) |



## kettle常用其它组件

###  Kettle转换组件

* 转换是ETL的T，T就是Transform清洗、转换

* ETL三个部分中，T花费时间最长,是“一般情况下这部分工作量是整个ETL的2/3

  | kettle转换组件                                   |
  | ------------------------------------------------ |
  | <img src="img/1569672821580.png" align="left" /> |

  



####  值映射

* 值映射就是把字段的一个值映射成其他的值
* 在数据质量规范上使用非常多，比如很多系统对应性别gender字段的定义不同
  * 系统1：1 男、2女
  * 系统2：f 男、m 女
  * 数据仓库统一为：male 男、female女



需求：

- 从user.json 中读取数据，并把gender列
  - 0 -> 男
  - 1 -> 女
  - 2 -> 保密
- 写入到Excel文件



实现步骤：

1、拖入一个 JSON输入组件、一个值映射转换组件、一个Excel输出组件，连接三个组件

2、配置JSON输入组件

3、配置值映射转换组件

4、配置Excel输出组件



具体实现：

1、拖入一个 JSON输入组件、一个值映射转换组件、一个Excel输出组件，连接三个组件

| 组件配置图                              |
| --------------------------------------- |
| ![1570415691415](img/1570415691415.png) |



2、配置JSON输入组件

3、配置值映射转换组件

| 值映射组件                                       |
| ------------------------------------------------ |
| <img src="img/1570415717712.png" align="left" /> |

4、配置Excel输出组件



####  增加序列

* 增加序列就是给数据流增加一个序列字段

  | 增加序列                                         |
  | ------------------------------------------------ |
  | <img src="img/1569674394571.png" align="left" /> |

  



需求：

* 从 user.json 读取数据，并添加序列，把数据保存到Excel

实现步骤：

1、拖入JSON输入组件、增加序列组件、Excel输出组件，并连接三个组件

| 组件配置图                              |
| --------------------------------------- |
| ![1570416388533](img/1570416388533.png) |



2、配置JSON Input组件

3、配置增加序列组件

| 配置增加序列                                     |
| ------------------------------------------------ |
| <img src="img/1570416417532.png" align="left" /> |



4、配置Excel输出组件



#### 字段选择

* 字段选择是从数据流中选择字段、改变名称、修改数据类型

需求：

- 从 user.json 中读取数据
- 移除birthday和register_date
- 把phone列名改为telephone，id列名改为key，gender列名改为sex
- 输出到Excel文件中



实现步骤：
1、拖入 JSON输入 组件、字段选择组件、Excel输出组件

| 组件配置图                              |
| --------------------------------------- |
| ![1570416693317](img/1570416693317.png) |

2、配置输入、字段选择、输出组件

| 配置输入，字段选择，输出组件            |
| --------------------------------------- |
| ![1570416732572](img/1570416732572.png) |



### Kettle流程控件

- 流程主要用来控制数据流程和数据流向

####  switch case

- switch/case组件让数据流从一路到多路。

  | switch case                                                  |
  | ------------------------------------------------------------ |
  | <img src="img/1569750489382.png" align="left" style="border:1px solid #999" /> |

  





需求：

- 从 user.json 输入读取数据，按sex进行数据分类，把女性、男性、保密分别保存不同的Excel文件里面。
  - 0表示男性
  - 1表示女性
  - 2表示保密

实现步骤：

1、拖入 JSON输入组件，switch/case组件，三个Excel输出组件

| 组件配置图                                            |
| ----------------------------------------------------- |
| ![1570429015330](img/1570429015330-1570494615088.png) |



2、配置 switch/case 组件

| 配置 switch/case 组件                                 |
| ----------------------------------------------------- |
| ![1570429033924](img/1570429033924-1570494615088.png) |





#### 过滤记录

过滤记录让数据流从一路到两路。

| 过滤记录                                                     |
| ------------------------------------------------------------ |
| <img src="img/1569750782651.png" align="left" style="border:1px solid #999"> |





需求：

- 从 user.json 读取数据，分离出 年龄 大于等于25，小于25的数据，分别保存到不同的Excel文件



实现步骤：

1、拖入 JSON输入组件、过滤记录组件、两个Excel组件，并连接各个组件

| 组件配置图                                            |
| ----------------------------------------------------- |
| ![1570345185863](img/1570345185863-1570494615067.png) |



2、配置过滤记录组件

| 配置过滤组件                                     |
| ------------------------------------------------ |
| <img src="img/1570345267807.png" align="left" /> |



##  Kettle作业和参数

###  Job（作业）

大多数ETL项目都需要完成各种各样的操作，例如：

* 如何传送文件
* 验证数据库表是否存在，等等

而这些操作都是按照一定顺序完成，Kettle中的作业可以串行执行转换来处理这些操作。

| 配置作业                                |
| --------------------------------------- |
| ![1570350109942](img/1570350109942.png) |





####  Job Item（作业项）

**作业项**是作业的基本构成部分。如同转换的组件，作业项也可以用**图标的方式**展示。

| 作业展示图                              |
| --------------------------------------- |
| ![1570365028328](img/1570365028328.png) |





* 作业顺序执行作业项，必须定义一个**起点**
* 有一个「start」的作业项专门用来定义起点
* 一个作业只能定一个开始作业项



####  Job Hop（作业跳）

Job Hop是作业项之间的连接线，定义了作业的执行路径，作业里每个作业项的不同运行结果决定了作业的不同执行路径。以下为 Job Hop的几种执行方式：

1、无条件执行

* 不论上一个作业项执行成功还是失败，下一个作业项都会执行

* 蓝色的连接线，上面有一个锁的图标

  | 无条件执行                                       |
  | ------------------------------------------------ |
  | <img src="img/1570365475738.png" align="left" /> |

  

2、当运行结果为真时执行

* 当上一个作业项的执行结果为真时，执行下一个作业项

* 通常在需要无错误执行的情况下使用

* 绿色的连接线，上面有一个对钩号的图标。

  | 结果为真执行                                     |
  | ------------------------------------------------ |
  | <img src="img/1570365533247.png" align="left" /> |

  



3、当运行结果为假时执行

* 当上一个作业项的执行结果为假或者没有成功执行时，执行下一个作业项
* 红色的连接线，上面有一个红色的停止图标

| 当结果为假时执行                                 |
| ------------------------------------------------ |
| <img src="img/1570365585789.png" align="left" /> |



####  作业示例

需求：

* 先从 `资料\作业数据源\01Excel输入.xlsx`读取数据，保存到Excel
* 再从 `资料\作业数据源\01文本文件输入.txt` 文本文件中读取数据，保存到Excel
* 启动作业执行
  * 执行错误，显示执行错误消息框
  * 执行成功，显示执行成功消息框



实现步骤：

1、设计转换结构1（从Excel读取数据，保存到Excel）

| 从Excel读取数据，保存到Excel                     |
| ------------------------------------------------ |
| <img src="img/1570366773214.png" align="left" /> |
| 从文本文件中读取数据，保存到Excel                |
| <img src="img/1570366794701.png" align="left"/>  |



3、设计作业结构（先执行转换结构1、再执行转换结构2）

| 作业结构图                              |
| --------------------------------------- |
| ![1570366831147](img/1570366831147.png) |



4、运行测试

5、错误测试

| 将第一个转换结构直接终止，并配置抛出一个错误     |
| ------------------------------------------------ |
| ![1570366711199](img/1570366711199.png)          |
| <img src="img/1570366744849.png" align="left" /> |



### 2 参数

####  参数的使用

对于ETL参数传递是一个很重要的环节，因为参数的传递会涉及到业务数据是如何抽取

####  转换命名参数

* 转换命名参数就是在转换内部定义的变量，作用范围是在转换内部

* 在转换的空白处双击左键，在转换属性中能看到

* 可以在表输入 SQL语句中使用 ${变量名} 或者 %%变量名%% 直接引用

  | 转换命名参数                            |
  | --------------------------------------- |
  | ![1570368608385](img/1570368608385.png) |

  

需求：

* 设置转换命名参数 default_province = 北京市
* 从t_user表中获取数据，满足条件 province=default_province，后续不要执行任何操作

实现步骤：

1、设计以下转换组件结构图

| 转换结构图                                       |
| ------------------------------------------------ |
| <img src="img/1570368865960.png" align="left" /> |



2、配置转换命名参数

| 设置转换命名参数                        |
| --------------------------------------- |
| ![1570369292238](img/1570369292238.png) |



3、配置表输入组件

| 配置表输入组件                          |
| --------------------------------------- |
| ![1570369270496](img/1570369270496.png) |



4、执行转换

| 执行转焕                                         |
| ------------------------------------------------ |
| <img src="img/1570369180659.png" align="left" /> |



## Kettle Linux部署

| kettle linux部署                        |
| --------------------------------------- |
| ![1570583989196](img/1570583989196.png) |



###  Linux安装Kettle

1、用File Zilla将kettle上传到Linux服务器，并解压缩

2、在命令行执行 

```shell
./pan.sh -version
./kitchen.sh -version
```

3、如果能够看到以下输出，表示kettle可以正确运行

```shell
2019/10/09 08:49:09 - Pan - Kettle version 8.2.0.0-342, build 8.2.0.0-342, build date : 2018-11-14 10.30.55
2019/10/09 08:49:09 - Pan - Start of run.
ERROR: No repository provided, can't load transformation.
```



```shell
2019/10/09 08:13:21 - Kitchen - Kettle version 8.2.0.0-342, build 8.2.0.0-342, build date : 2018-11-14 10.30.55
2019/10/09 08:13:21 - Kitchen - Start of run.
ERROR: Kitchen can't continue because the job couldn't be loaded.
```



4、配置环境变量

```shell
# KETTLE
export KETTLE=/export/softwares/data-integration
export PATH=${KETTLE}:$PATH
```





###  Pan——转换执行引擎

pan.sh可以用来在服务器中执行一个转换

pan.sh的命令行参数:

```shel
-version：显示版本信息
-file: 指定要运行的转换文件（XML文件）
-level: 设置日志级别(Basic,Detailed,Debug,Rowlevel,Error,Nothing)
-log: 指定日志文件
-param:key=value （该参数可以指定多个）覆盖之前指定的默认的命名参数
```



需求：

* 在Linux中，将 /root/kettle/user.json 数据抽取到 /root/kettle/user.xls 表格中



实现步骤：

1、在 windows 中开发转换，将 json数据抽取装载到 user.xls文件中

2、抽取路径参数，通过命令行指定 json数据文件路径，指定 user.xls 文件路径

| 设置转换命名参数                                       |
| ------------------------------------------------------ |
| ![1570583639139](img/1570583639139.png)                |
| ![1570583639139](img/1570583639139-16468852928323.png) |
| ![1570583672960](img/1570583672960.png)                |
| ![1570583692029](img/1570583692029.png)                |





3、将数据文件上传到 /root/kettle 目录

4、上传转换文件、json数据文件到Linux服务器

5、使用 pan.sh 执行转换

```shell
pan.sh -file 8.transform_param.ktr -level Basic -param:input=/root/kettle/user.json -param:output=/root/kettle/output_user
```



### Kitchen——作业执行引擎

在Linux中，可以使用 kitchen.sh 来执行作业

需求：

* 执行JSON数据抽取到Excel中

实现步骤：

1、在windows中开发作业

| 作业配置图                              |
| --------------------------------------- |
| ![1574402751092](img/1574402751092.png) |





2、配置转换组件

| 引入之前定义好的转换任务                |
| --------------------------------------- |
| ![1574402829839](img/1574402829839.png) |





3 windows本地测试执行

4 、修改转换中的路径参数改为用变量来接收

| windows测试                                                 |
| ----------------------------------------------------------- |
| ![image-20200206181718257](img/image-20200206181718257.png) |



5、配置作业命名参数

| 作业的命名参数                          |
| --------------------------------------- |
| ![1570583550498](img/1570583550498.png) |



6、启动测试执行

6、上传JOB文件到Linux服务器的`/root/kettle/`目录

7、使用kitchen.sh执行作业

```shell
kitchen.sh -file job_transform.kjb -level Basic -param:input=/root/kettle/user.json -param:output=/root/kettle/output_user
```

# 